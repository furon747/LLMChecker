{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "# This will just have to be changed when you run the file\n",
    "PATH = \"C:\\\\Users\\\\Chris\\\\Desktop\\\\CIS 583 Final Project\\\\data\\\\training_data.csv\"\n",
    "Distinct_words = []\n",
    "Data = []\n",
    "Tags = []       #  0 = written by student, 1 = generated by LLM\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "def parse_ham_files():\n",
    "    global Data\n",
    "    if(os.path.isfile(PATH)):\n",
    "        with open(PATH, 'r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                if(len(row) < 2):\n",
    "                    continue\n",
    "\n",
    "                type, content = row[0], row[1]\n",
    "                parsed_content = clean_content(content)     # maybe don't clean data?\n",
    "                Data.append(parsed_content)\n",
    "\n",
    "                ## TODO: If model is acting funky, check if our code is mixing up 0s and 1s, since before 0 was spam, and 1 was ham. In this case it is the opposite (0 = human, 1 = LLM)\n",
    "                if(type == \"0\"):\n",
    "                    Tags.append(0)\n",
    "                else:\n",
    "                    Tags.append(1)\n",
    "\n",
    "def clean_content(content):\n",
    "    parsed_content = content.lower()\n",
    "    translator = str.maketrans('','', string.punctuation)\n",
    "    parsed_content = parsed_content.translate(translator)\n",
    "\n",
    "    if(\"\\n\" in parsed_content):\n",
    "        for word in parsed_content:\n",
    "            if(\"\\n\" in word):\n",
    "                word = word.replace(\"\\n\", \" \")\n",
    "\n",
    "    return parsed_content\n",
    "\n",
    "def get_distinct_words():\n",
    "    global Distinct_words\n",
    "    Distinct_words_set = set(Distinct_words)\n",
    "    for item in Data:\n",
    "        Distinct_words_set.update(item.split())\n",
    "    \n",
    "    Distinct_words = list(Distinct_words_set)\n",
    "\n",
    "def split_data(Data, Tags, X_train, X_test, y_train, y_test):\n",
    "    combined_list = list(zip(Data, Tags))\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(combined_list)\n",
    "    Data, Tags = zip(*combined_list)\n",
    "\n",
    "    Data = list(Data)\n",
    "    Tags = list(Tags)\n",
    "\n",
    "    data_midpoint = int(len(Data) / 2)\n",
    "    \n",
    "    X_train = np.array(Data[:data_midpoint])\n",
    "    X_test = np.array(Data[data_midpoint:])\n",
    "\n",
    "    y_train = np.array(Tags[:data_midpoint])\n",
    "    y_test = np.array(Tags[data_midpoint:])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# much faster\n",
    "\n",
    "def to_data_matrix_v2(Distinct_words, data):\n",
    "    vectorizer = CountVectorizer(vocabulary=Distinct_words)\n",
    "    data_matrix = vectorizer.transform(data).toarray()\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94079\n"
     ]
    }
   ],
   "source": [
    "parse_ham_files()\n",
    "get_distinct_words()\n",
    "print(len(Distinct_words))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(Data,Tags, X_train, X_test, y_train, y_test)\n",
    "training_matrix = to_data_matrix_v2(Distinct_words, X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrix(matrix, row_index):\n",
    "    for i in range(row_index):\n",
    "        line = \"\"\n",
    "        for j in range(len(matrix)):\n",
    "            line = line + (f\"{matrix[i][j]}, \")\n",
    "        print(line)\n",
    "\n",
    "#print_matrix(training_matrix, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaussianNB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted 0.8777356808558057 samples correctly\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the model\n",
    "model = GaussianNB()\n",
    "model.fit(training_matrix, y_train)\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=Distinct_words)\n",
    "\n",
    "X_test_matrix = vectorizer.transform(X_test).toarray()\n",
    "predictions = model.predict(X_test_matrix)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Model predicted {accuracy} samples correctly\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      0.85      0.89     13692\n",
      "     Class 1       0.80      0.92      0.85      8743\n",
      "\n",
      "    accuracy                           0.88     22435\n",
      "   macro avg       0.87      0.89      0.87     22435\n",
      "weighted avg       0.89      0.88      0.88     22435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_predictions = model.predict(X_test_matrix)\n",
    "print(classification_report(y_test, test_predictions, target_names=[\"Class 0\", \"Class 1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "new_data = [\"Detecting whether a message was generated by a large language model (LLM) or is organic (human-created) requires a model or approach specifically designed to differentiate between the two. The best model depends on your requirements, such as the type of LLM (e.g., GPT, Bard), the dataset, and your computational resources. Here are some options:\"]\n",
    "new_data_matrix = vectorizer.transform(new_data).toarray()\n",
    "pred = model.predict(new_data_matrix)\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.99019389 0.99219969 0.99108536 0.98997103 0.98885421]\n",
      "Mean score: 0.9904608359355074\n",
      "Model predicted 0.9898818809895253 samples correctly\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = LinearSVC(random_state=42)\n",
    "clf.fit(training_matrix, y_train)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=Distinct_words)\n",
    "\n",
    "X_test_matrix = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "\n",
    "scores = cross_val_score(clf, training_matrix, y_train, cv=5)  # 5-fold cross-validation\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean score: {scores.mean()}\")\n",
    "\n",
    "\n",
    "predictions = clf.predict(X_test_matrix)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Model predicted {accuracy} samples correctly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.99      0.99      0.99     13692\n",
      "     Class 1       0.99      0.99      0.99      8743\n",
      "\n",
      "    accuracy                           0.99     22435\n",
      "   macro avg       0.99      0.99      0.99     22435\n",
      "weighted avg       0.99      0.99      0.99     22435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_predictions = clf.predict(X_test_matrix)\n",
    "print(classification_report(y_test, test_predictions, target_names=[\"Class 0\", \"Class 1\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive-Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted 0.9606864274570983 samples correctly\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(training_matrix, y_train)\n",
    "\n",
    "X_test_matrix = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "predictions = mnb.predict(X_test_matrix)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Model predicted {accuracy} samples correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96010698 0.96300423 0.96211277 0.96211277 0.9560856 ]\n",
      "Mean score: 0.9606844700512432\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(mnb, training_matrix, y_train, cv=5)  # 5-fold cross-validation\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean score: {scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      0.99      0.97     13692\n",
      "     Class 1       0.98      0.92      0.95      8743\n",
      "\n",
      "    accuracy                           0.96     22435\n",
      "   macro avg       0.96      0.95      0.96     22435\n",
      "weighted avg       0.96      0.96      0.96     22435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_predictions = mnb.predict(X_test_matrix)\n",
    "print(classification_report(y_test, test_predictions, target_names=[\"Class 0\", \"Class 1\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "X_train = X_train.tolist()\n",
    "# Tokenize the data\n",
    "encodings = tokenizer(\n",
    "    X_train,\n",
    "    truncation=True,  # Truncate sequences to max length\n",
    "    padding=\"max_length\",  # Pad sequences to uniform length\n",
    "    max_length=10,  # Maximum length of sequences\n",
    "    return_tensors=\"pt\"  # Return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "labels = torch.tensor(y_train, dtype=torch.long)\n",
    "train_dataset = TextDataset(encodings, labels)\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1403/1403 [08:29<00:00,  2.75it/s, loss=0.16]  \n",
      "Epoch 1: 100%|██████████| 1403/1403 [09:55<00:00,  2.36it/s, loss=0.00153]\n",
      "Epoch 2: 100%|██████████| 1403/1403 [08:36<00:00,  2.72it/s, loss=0.0132] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # Move data to GPU/CPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(\n",
    "    X_test.tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Create a test dataset and DataLoader\n",
    "test_labels = torch.tensor(y_test, dtype=torch.long)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8779\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.9130\n",
      "Macro Recall: 0.8445\n",
      "Macro F1-Score: 0.8622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute macro-average metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\n",
    "print(f\"Macro Precision: {precision:.4f}\")\n",
    "print(f\"Macro Recall: {recall:.4f}\")\n",
    "print(f\"Macro F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.84      1.00      0.91     13692\n",
      "     Class 1       0.99      0.69      0.82      8743\n",
      "\n",
      "    accuracy                           0.88     22435\n",
      "   macro avg       0.91      0.84      0.86     22435\n",
      "weighted avg       0.90      0.88      0.87     22435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(all_labels, all_preds, target_names=[\"Class 0\", \"Class 1\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: There is no one-size-fits-all answer to whether overwriting old data in a data warehouse is best practice. It depends on factors such as data retention policies, data quality requirements, storage considerations, compliance obligations, and business use cases. Further, overwriting old data increases the risk of losing valuable historical information, so it's important to have processes in place to recover data if needed. It's essential to carefully evaluate these factors and implement a data management strategy that aligns with your organization's needs and objectives.\n",
      "\n",
      "Also, matching new data to old data in a data warehouse when they are from two different platforms can be challenging but feasible with the right approach. Some of the strategies used would be:\n",
      "\n",
      "Standardization of Data Formats: Ensure that data from both platforms are in a standardized format or can be transformed into a common format. This might involve converting data types, standardizing naming conventions, and ensuring consistent structures.\n",
      "\n",
      "Data Integration Tools: Utilize data integration tools or ETL (Extract, Transform, Load) processes to bring data from both platforms into the data warehouse. These tools often have features to handle data mapping, transformation, and loading from different sources.\n",
      "\n",
      "Unique Identifiers: Identify unique identifiers in both datasets that can be used for matching records. This might be unique keys that exist in both platforms.\n",
      "\n",
      "Data Cleansing and Deduplication: Perform data cleansing to remove inconsistencies and errors in both datasets. Deduplicate records to ensure that each record is unique within the dataset.\n",
      "\n",
      "Data Matching Algorithms: Implement data matching algorithms to compare records from the two datasets and identify matching or similar records. These algorithms can be based on various criteria such as similarity of attributes, fuzzy matching, or other matching rules.\n",
      "\n",
      "Manual Review and Validation: Depending on the criticality and complexity of the data, manual review and validation may be necessary to ensure the accuracy of matching results. This can involve human judgment to resolve ambiguities or edge cases.\n",
      "\n",
      "Incremental Updates: Implement mechanisms for incremental updates to the data warehouse to keep it synchronized with new data from both platforms. This might involve periodic data refreshes or real-time data streaming, depending on the frequency and volume of new data.\n",
      "\n",
      "Monitoring and Maintenance: Regularly monitor the data matching process and perform maintenance activities to address any issues that arise. This includes updating matching algorithms, resolving data quality issues, and adapting to changes in the source platforms.\n",
      "\n",
      "By following these steps and leveraging appropriate technologies, you can effectively match new data to old data in a data warehouse, even when they originate from different platforms.\n",
      "\n",
      "Finally, while both data warehouses and data lakes are used for storing and managing data, they aren't the same as they differ in their architecture, purpose, and the types of data they handle. Data warehouses are optimized for structured data and analytics, while data lakes are designed for storing raw data of various types for flexible analysis and processing.\n",
      "\n",
      "Some of the points to consider while weighing the pros and cons of a Data lake and a Data warehouse would be:\n",
      "\n",
      "a) Flexibility: Data lakes offer greater flexibility in terms of storing raw, unstructured data. If you have diverse data sources and want to store data without predefined schemas, a data lake might be more suitable.\n",
      "\n",
      "b) Scalability: Data lakes are designed to scale horizontally and can handle massive volumes of data. They are well-suited for big data applications where scalability is a key requirement.\n",
      "\n",
      "c) Cost: Data lakes can be more cost-effective for storing large volumes of raw data compared to traditional data warehouses, especially when dealing with unstructured data.\n",
      "\n",
      "d) Data Governance and Quality: Data warehouses typically enforce strict governance and quality controls, ensuring data consistency and accuracy. Data lakes may require more effort in terms of data governance and quality assurance since they store raw, unstructured data.\n",
      "I am 95.6561% confident this message was written by an LLM\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# For context, this was a discussion board reply to one of my previous course's (CIS 556) posts.\n",
    "# As I suspected but couldn't prove at the time, this was likely written by an LLM.\n",
    "new_texts = [\"\"\"There is no one-size-fits-all answer to whether overwriting old data in a data warehouse is best practice. It depends on factors such as data retention policies, data quality requirements, storage considerations, compliance obligations, and business use cases. Further, overwriting old data increases the risk of losing valuable historical information, so it's important to have processes in place to recover data if needed. It's essential to carefully evaluate these factors and implement a data management strategy that aligns with your organization's needs and objectives.\n",
    "\n",
    "Also, matching new data to old data in a data warehouse when they are from two different platforms can be challenging but feasible with the right approach. Some of the strategies used would be:\n",
    "\n",
    "Standardization of Data Formats: Ensure that data from both platforms are in a standardized format or can be transformed into a common format. This might involve converting data types, standardizing naming conventions, and ensuring consistent structures.\n",
    "\n",
    "Data Integration Tools: Utilize data integration tools or ETL (Extract, Transform, Load) processes to bring data from both platforms into the data warehouse. These tools often have features to handle data mapping, transformation, and loading from different sources.\n",
    "\n",
    "Unique Identifiers: Identify unique identifiers in both datasets that can be used for matching records. This might be unique keys that exist in both platforms.\n",
    "\n",
    "Data Cleansing and Deduplication: Perform data cleansing to remove inconsistencies and errors in both datasets. Deduplicate records to ensure that each record is unique within the dataset.\n",
    "\n",
    "Data Matching Algorithms: Implement data matching algorithms to compare records from the two datasets and identify matching or similar records. These algorithms can be based on various criteria such as similarity of attributes, fuzzy matching, or other matching rules.\n",
    "\n",
    "Manual Review and Validation: Depending on the criticality and complexity of the data, manual review and validation may be necessary to ensure the accuracy of matching results. This can involve human judgment to resolve ambiguities or edge cases.\n",
    "\n",
    "Incremental Updates: Implement mechanisms for incremental updates to the data warehouse to keep it synchronized with new data from both platforms. This might involve periodic data refreshes or real-time data streaming, depending on the frequency and volume of new data.\n",
    "\n",
    "Monitoring and Maintenance: Regularly monitor the data matching process and perform maintenance activities to address any issues that arise. This includes updating matching algorithms, resolving data quality issues, and adapting to changes in the source platforms.\n",
    "\n",
    "By following these steps and leveraging appropriate technologies, you can effectively match new data to old data in a data warehouse, even when they originate from different platforms.\n",
    "\n",
    "Finally, while both data warehouses and data lakes are used for storing and managing data, they aren't the same as they differ in their architecture, purpose, and the types of data they handle. Data warehouses are optimized for structured data and analytics, while data lakes are designed for storing raw data of various types for flexible analysis and processing.\n",
    "\n",
    "Some of the points to consider while weighing the pros and cons of a Data lake and a Data warehouse would be:\n",
    "\n",
    "a) Flexibility: Data lakes offer greater flexibility in terms of storing raw, unstructured data. If you have diverse data sources and want to store data without predefined schemas, a data lake might be more suitable.\n",
    "\n",
    "b) Scalability: Data lakes are designed to scale horizontally and can handle massive volumes of data. They are well-suited for big data applications where scalability is a key requirement.\n",
    "\n",
    "c) Cost: Data lakes can be more cost-effective for storing large volumes of raw data compared to traditional data warehouses, especially when dealing with unstructured data.\n",
    "\n",
    "d) Data Governance and Quality: Data warehouses typically enforce strict governance and quality controls, ensuring data consistency and accuracy. Data lakes may require more effort in terms of data governance and quality assurance since they store raw, unstructured data.\"\"\"]\n",
    "new_encodings = tokenizer(\n",
    "    new_texts,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    input_ids = new_encodings[\"input_ids\"].to(device)\n",
    "    attention_mask = new_encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "        # Compute probabilities with softmax\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "    predictions = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
    "    confidences = torch.max(probabilities, dim=1).values.cpu().numpy()\n",
    "\n",
    "for text, label, confidence in zip(new_texts, predictions, confidences):\n",
    "    print(f\"Text: {text}\")\n",
    "    #print(f\"Predicted Label: {label}, Confidence: {confidence:.4f}\\n\")\n",
    "    if(label == 0):\n",
    "        print(f\"I am {(confidence * 100):.4f}% confident this message was written by a human\")\n",
    "    else:\n",
    "        print(f\"I am {(confidence * 100):.4f}% confident this message was written by an LLM\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
